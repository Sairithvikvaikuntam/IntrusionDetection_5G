{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "#to ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "-Jc6ytvmKcKF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4d3c0fe-a2b7-4c7f-b923-cc859f867166"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4UQAM4XZFnsG"
      },
      "outputs": [],
      "source": [
        "def data_preprocessing_binary(path):\n",
        "  df = pd.read_csv(path)\n",
        "  df_encoded = pd.get_dummies(df[['Proto','State']], columns=['Proto', 'State'])\n",
        "\n",
        "  select_features_df = df[[ 'AckDat', 'sHops', 'Seq','TcpRtt', 'dMeanPktSz', 'Offset', 'sTtl',  'Mean', 'SrcTCPBase', 'sMeanPktSz', 'DstLoss', 'Loss', 'dTtl', 'SrcBytes', 'TotBytes']]\n",
        "  select_features_df['sHops'].fillna(df['sHops'].mean(), inplace = True)\n",
        "  select_features_df['sTtl'].fillna(df['sTtl'].mean(), inplace = True)\n",
        "  select_features_df['SrcTCPBase'].fillna(df['SrcTCPBase'].mean(), inplace = True)\n",
        "  select_features_df['dTtl'].fillna(df['dTtl'].mean(), inplace = True)\n",
        "  df_merged = pd.concat([select_features_df, df_encoded], axis=1)\n",
        "\n",
        "  df_output_encoded = df['Label']\n",
        "  df_output_encoded_1 = df_output_encoded.replace(['Benign', 'Malicious'],\n",
        "                        [0, 1], inplace=False)\n",
        "\n",
        "\n",
        "  #returns input and output dataframe\n",
        "  return df_merged, df_output_encoded_1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_cnn(X, y):\n",
        "  import tensorflow as tf\n",
        "  from tensorflow.keras import layers, models\n",
        "  from sklearn.model_selection import train_test_split\n",
        "  import numpy as np\n",
        "  from tensorflow.keras.models import Sequential\n",
        "  from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "  from tensorflow.keras.optimizers import Adam\n",
        "  from tensorflow.keras.metrics import Precision, Recall\n",
        "  from sklearn.metrics import f1_score\n",
        "  import time\n",
        "\n",
        "\n",
        "  # Assuming df_normalized is your preprocessed DataFrame\n",
        "  # Assuming df_output_encoded_1 contains the binary labels for classification\n",
        "\n",
        "  # Split the data into features and labels\n",
        "  X = X.values  # Features\n",
        "  y = y.values  # Labels\n",
        "\n",
        "  # Split the data into training and testing sets\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "  # Reshape the input data for CNN (assuming df_normalized has shape (n_samples, n_features))\n",
        "  input_shape = (X.shape[1], 1)  # Assuming you have n_features columns\n",
        "  X_train = X_train.reshape(-1, X_train.shape[1], 1)\n",
        "  X_test = X_test.reshape(-1, X_test.shape[1], 1)\n",
        "\n",
        "  # Define the CNN architecture\n",
        "  model = models.Sequential([\n",
        "      # Convolutional layers\n",
        "      layers.Conv1D(32, 3, activation='relu', input_shape=input_shape),\n",
        "      layers.MaxPooling1D(2),\n",
        "      layers.Conv1D(64, 3, activation='relu'),\n",
        "      layers.MaxPooling1D(2),\n",
        "      layers.Conv1D(64, 3, activation='relu'),\n",
        "\n",
        "      # Dense layers\n",
        "      layers.Flatten(),\n",
        "      layers.Dense(64, activation='relu'),\n",
        "      layers.Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
        "  ])\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(optimizer='adam',\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy', Precision(), Recall()])\n",
        "\n",
        "  start_time = time.time()\n",
        "  history = model.fit(X_train, y_train, epochs=1, batch_size=32, validation_data=(X_test, y_test))\n",
        "  end_time = time.time()\n",
        "  training_time = end_time - start_time\n",
        "\n",
        "  # Evaluate the model\n",
        "  loss, accuracy, precision, recall = model.evaluate(X_test, y_test)\n",
        "  y_pred = model.predict(X_test)\n",
        "  f1 = f1_score(y_test, y_pred > 0.5)\n",
        "\n",
        "  # Print metrics\n",
        "  print(\"Model Loss:\", loss)\n",
        "  print(\"Model Accuracy:\", accuracy)\n",
        "  print(\"Model Precision:\", precision)\n",
        "  print(\"Model Recall:\", recall)\n",
        "  print(\"F1 Score:\", f1)\n",
        "  print(\"Training Time:\", training_time)\n",
        "\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "4tCEuLOmOK5k"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_cnn_numpyarr(X, y):\n",
        "    # Import necessary libraries\n",
        "    import tensorflow as tf\n",
        "    from tensorflow.keras import layers, models\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.metrics import f1_score\n",
        "    import time\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Check the shape of X_train\n",
        "    print(\"Shape of X_train:\", X_train.shape)\n",
        "\n",
        "    # Reshape the input data for CNN (assuming X has shape (n_samples, n_features))\n",
        "    input_shape = (X_train.shape[1], 1)  # Assuming you have n_features columns\n",
        "    X_train = X_train.reshape(-1, X_train.shape[1], 1)\n",
        "    X_test = X_test.reshape(-1, X_test.shape[1], 1)\n",
        "\n",
        "    # Define the CNN architecture\n",
        "    model = models.Sequential([\n",
        "        # Convolutional layers\n",
        "        layers.Conv1D(32, 3, activation='relu', input_shape=input_shape),\n",
        "        layers.MaxPooling1D(2),\n",
        "        layers.Conv1D(64, 3, activation='relu'),\n",
        "\n",
        "        # Dense layers\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy', tf.metrics.Precision(), tf.metrics.Recall()])\n",
        "\n",
        "    # Start training time measurement\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(X_train, y_train, epochs=1, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "    # End training time measurement\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Calculate training time\n",
        "    training_time = end_time - start_time\n",
        "\n",
        "    # Evaluate the model\n",
        "    loss, accuracy, precision, recall = model.evaluate(X_test, y_test)\n",
        "    y_pred = model.predict(X_test)\n",
        "    f1 = f1_score(y_test, y_pred > 0.5)\n",
        "\n",
        "    # Print metrics\n",
        "    print(\"Model Loss:\", loss)\n",
        "    print(\"Model Accuracy:\", accuracy)\n",
        "    print(\"Model Precision:\", precision)\n",
        "    print(\"Model Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1)\n",
        "    print(\"Training Time:\", training_time)\n",
        "\n",
        "    return model,X_test, y_test, history"
      ],
      "metadata": {
        "id": "TgasyOfScpxd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def standardize(input_df):\n",
        "  scaler = StandardScaler()\n",
        "  df_standardized = scaler.fit_transform(input_df)\n",
        "  df_standardized = pd.DataFrame(df_standardized, columns=input_df.columns)\n",
        "  return df_standardized\n"
      ],
      "metadata": {
        "id": "quHxJyo3LNhS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "def normalize_data(input_df):\n",
        "    # Handling missing values\n",
        "    imputer = SimpleImputer(strategy='mean')\n",
        "    input_df_filled = pd.DataFrame(imputer.fit_transform(input_df), columns=input_df.columns)\n",
        "\n",
        "    # Feature scaling\n",
        "    scaler = StandardScaler()\n",
        "    input_scaled = pd.DataFrame(scaler.fit_transform(input_df_filled), columns=input_df.columns)\n",
        "\n",
        "    return input_scaled\n"
      ],
      "metadata": {
        "id": "0yL_1Y9RzK72"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "\n",
        "def remove_outliers_zscore(input_df, threshold=3):\n",
        "    z_scores = np.abs(stats.zscore(input_df))\n",
        "    input_df_no_outliers = input_df[(z_scores < threshold).all(axis=1)]\n",
        "    return input_df_no_outliers"
      ],
      "metadata": {
        "id": "2Q7nkyigeVx0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "def select_features(input_df, target, k=10):\n",
        "    selector = SelectKBest(score_func=f_classif, k=k)\n",
        "    X_selected = selector.fit_transform(input_df, target)\n",
        "    selected_features = input_df.columns[selector.get_support()]\n",
        "    return X_selected, selected_features"
      ],
      "metadata": {
        "id": "SYiUEgx8YxZt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve, auc\n",
        "import itertools\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, classes):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.colorbar()\n",
        "    tick_marks = range(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], 'd'),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()"
      ],
      "metadata": {
        "id": "FXXQRGl3oRyR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    y_pred_probs = model.predict(X_test)\n",
        "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "    plot_confusion_matrix(y_test, y_pred, classes=['Benign', 'Malicious'])\n"
      ],
      "metadata": {
        "id": "IuQ2x4yBo4CG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/5G NIDD/Combined.csv'\n",
        "input, output = data_preprocessing_binary(path)\n"
      ],
      "metadata": {
        "id": "Qk4eMbz9JkYx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_standardized = standardize(input)"
      ],
      "metadata": {
        "id": "8wevgV2jKwpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalize_data = normalize_data(input)"
      ],
      "metadata": {
        "id": "BuCBL-6Tf0ay"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_no_outliers = remove_outliers_zscore(normalize_data)"
      ],
      "metadata": {
        "id": "TmX8oG7dfJA-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = output.iloc[input_no_outliers.index]"
      ],
      "metadata": {
        "id": "znoiUDMkiAee"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_selected, selected_features = select_features(input_no_outliers, output)"
      ],
      "metadata": {
        "id": "VcLE2veYZCk4"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"input_select:\",pd.DataFrame(input_selected).head())"
      ],
      "metadata": {
        "id": "s2L7cDDCijnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model = binary_cnn(input_standardized, output)"
      ],
      "metadata": {
        "id": "Ks3CM1pTMh_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model,X_test, y_test, history = binary_cnn_numpyarr(input_selected, output)"
      ],
      "metadata": {
        "id": "AsGq-Ckgzhvf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3594fe8e-7711-428e-8ff4-a3bb53c96bab"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train: (738972, 10)\n",
            "23093/23093 [==============================] - 77s 3ms/step - loss: 0.0101 - accuracy: 0.9975 - precision: 0.9971 - recall: 0.9986 - val_loss: 0.0021 - val_accuracy: 0.9995 - val_precision: 0.9997 - val_recall: 0.9994\n",
            "5774/5774 [==============================] - 11s 2ms/step - loss: 0.0021 - accuracy: 0.9995 - precision: 0.9997 - recall: 0.9994\n",
            "5774/5774 [==============================] - 10s 2ms/step\n",
            "Model Loss: 0.002051039133220911\n",
            "Model Accuracy: 0.9994803667068481\n",
            "Model Precision: 0.9996948838233948\n",
            "Model Recall: 0.9993900060653687\n",
            "F1 Score: 0.9995424169915823\n",
            "Training Time: 83.28888130187988\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(cnn_model, X_test, y_test)"
      ],
      "metadata": {
        "id": "9IKqUCV2pOZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "def create_rnn_model(input_shape):\n",
        "    model = Sequential([\n",
        "        LSTM(64, input_shape=input_shape),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "F_yAiYSSd5gk"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming X_train and X_test are your input features\n",
        "\n",
        "# Reshape the input data for the RNN model\n",
        "X_train_rnn = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test_rnn = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n"
      ],
      "metadata": {
        "id": "Phkeb7sid7F2"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "# Define a custom transformer to incorporate RNN model into the pipeline\n",
        "class RNNWrapper(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, rnn_model):\n",
        "        self.rnn_model = rnn_model\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self.rnn_model.fit(X, y, epochs=10, batch_size=32)  # Train the RNN model\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return self.rnn_model.predict(X)\n"
      ],
      "metadata": {
        "id": "KLlKs5oid_Pe"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define and compile the RNN model\n",
        "rnn_model = create_rnn_model(input_shape=(1, X_train.shape[1]))\n",
        "rnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the RNN model\n",
        "rnn_model.fit(X_train_rnn, y_train, epochs=10, batch_size=32, validation_data=(X_test_rnn, y_test))\n"
      ],
      "metadata": {
        "id": "DF3onuiheBxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on test data using the trained RNN model\n",
        "y_pred = rnn_model.predict(X_test_rnn)\n"
      ],
      "metadata": {
        "id": "qqxike5_kDpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Threshold the predictions\n",
        "threshold = 0.5\n",
        "y_pred_binary = (y_pred > threshold).astype(int)\n",
        "\n",
        "# Print the first few binary predictions\n",
        "print(\"Sample of binary predictions:\")\n",
        "print(y_pred_binary[:10])\n"
      ],
      "metadata": {
        "id": "RuKdk3cIlv6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Evaluate the predictions\n",
        "accuracy = accuracy_score(y_test, y_pred_binary)\n",
        "precision = precision_score(y_test, y_pred_binary)\n",
        "recall = recall_score(y_test, y_pred_binary)\n",
        "f1 = f1_score(y_test, y_pred_binary)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Evaluation Metrics:\")\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n"
      ],
      "metadata": {
        "id": "6sQJFwlckc1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def binary_cnn_with_resnet(X_train, X_test, y_train, y_test, base_model):\n",
        "    from sklearn.metrics import f1_score\n",
        "    import time\n",
        "\n",
        "    # Define custom layers on top of ResNet\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(1024, activation='relu')(x)  # Add your custom dense layers\n",
        "    predictions = Dense(1, activation='sigmoid')(x)  # Output layer for binary classification\n",
        "\n",
        "    # Create the transfer learning model\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "    # Freeze the layers in the base ResNet model\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=Adam(),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # Start training time measurement\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train, epochs=1, batch_size=32, validation_data=(X_test, y_test), verbose=0)\n",
        "\n",
        "    # End training time measurement\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Calculate training time\n",
        "    training_time = end_time - start_time\n",
        "\n",
        "    # Evaluate the model\n",
        "    loss, accuracy, precision, recall = model.evaluate(X_test, y_test)\n",
        "    y_pred = model.predict(X_test)\n",
        "    f1 = f1_score(y_test, y_pred > 0.5)\n",
        "\n",
        "    # Print metrics\n",
        "    print(\"Model Loss:\", loss)\n",
        "    print(\"Model Accuracy:\", accuracy)\n",
        "    print(\"Model Precision:\", precision)\n",
        "    print(\"Model Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1)\n",
        "    print(\"Training Time:\", training_time)\n",
        "\n",
        "    return model, training_time\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "w_apPsDy_1us"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metrics(y_true, y_pred):\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    return accuracy, precision, recall, f1\n"
      ],
      "metadata": {
        "id": "owT-nXneVppX"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "# Define the machine learning pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('scaling', StandardScaler()),  # Standardize input features\n",
        "    ('model', None)  # Placeholder for the model\n",
        "])\n",
        "\n",
        "# Load the pre-trained ResNet50 model\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Update the model in the pipeline with the ResNet-based transfer learning model\n",
        "pipeline.named_steps['model'] = base_model\n",
        "\n",
        "# Assuming X and y are your input features and labels\n",
        "X, y = input_selected, output # Your data\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "\n",
        "X_train_resized = np.array([image.img_to_array(image.array_to_img(x).resize((224, 224))) for x in X_train])\n",
        "X_test_resized = np.array([image.img_to_array(image.array_to_img(x).resize((224, 224))) for x in X_test])\n",
        "\n",
        "# Preprocess input images\n",
        "X_train_resnet = preprocess_input(X_train_resized)\n",
        "X_test_resnet = preprocess_input(X_test_resized)\n",
        "\n",
        "print(\"Shape of X_train:\", X_train_resnet.shape)\n",
        "print(\"Shape of X_test:\", X_test_resnet.shape)\n",
        "\n",
        "# Fit the model within the pipeline\n",
        "model, training_time = binary_cnn_with_resnet(X_train_resnet, X_test_resnet, y_train, y_test, base_model)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test_resnet)\n",
        "y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy, precision, recall, f1 = calculate_metrics(y_test, y_pred_binary)\n",
        "\n",
        "# Print metrics\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "print(\"Model Precision:\", precision)\n",
        "print(\"Model Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "print(\"Training Time:\", training_time)\n"
      ],
      "metadata": {
        "id": "cLtO5NJUVv6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_preprocessing_multiclass(path):\n",
        "  from sklearn.preprocessing import LabelEncoder\n",
        "  df = pd.read_csv(path)\n",
        "  df_encoded = pd.get_dummies(df[['Proto','State']], columns=['Proto', 'State'])\n",
        "  df_encoded_output = pd.get_dummies(df[['Proto','State']], columns=['Proto', 'State'])\n",
        "  select_features_df = df[[ 'AckDat', 'sHops', 'Seq','TcpRtt', 'dMeanPktSz', 'Offset', 'sTtl',  'Mean', 'SrcTCPBase', 'sMeanPktSz', 'DstLoss', 'Loss', 'dTtl', 'SrcBytes', 'TotBytes']]\n",
        "  select_features_df['sHops'].fillna(df['sHops'].mean(), inplace = True)\n",
        "  select_features_df['sTtl'].fillna(df['sTtl'].mean(), inplace = True)\n",
        "  select_features_df['SrcTCPBase'].fillna(df['SrcTCPBase'].mean(), inplace = True)\n",
        "  select_features_df['dTtl'].fillna(df['dTtl'].mean(), inplace = True)\n",
        "  df_merged = pd.concat([select_features_df, df_encoded], axis=1)\n",
        "\n",
        "  # df_output_encoded = df['Label']\n",
        "  # df_output_encoded_1 = df_output_encoded.replace(['Benign', 'Malicious'],\n",
        "  #                       [0, 1], inplace=False)\n",
        "\n",
        "  label_encoder = LabelEncoder()\n",
        "  df_output_encoded = label_encoder.fit_transform(df['Attack Type'])\n",
        "\n",
        "  # Convert output encoded array into DataFrame\n",
        "  df_output_encoded_1 = pd.DataFrame(df_output_encoded, columns=['Attack_Type_Encoded'])\n",
        "  print(df_output_encoded_1)\n",
        "  #returns input and output dataframe\n",
        "  return df_merged, df_output_encoded_1"
      ],
      "metadata": {
        "id": "YDrVgftvTLWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def multiclass_cnn(X, y):\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    from tensorflow.keras import layers, models\n",
        "    from tensorflow.keras.utils import to_categorical\n",
        "    from tensorflow.keras.metrics import Precision, Recall\n",
        "    from sklearn.metrics import f1_score\n",
        "    import time\n",
        "\n",
        "    # Load the input data and labels\n",
        "    # Assuming X_input is your input DataFrame and y_output is your output DataFrame\n",
        "    X = X.values\n",
        "    y = y.values\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Standardize the input features\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    # Convert labels to integers and one-hot encode them\n",
        "    num_classes = len(np.unique(y))\n",
        "    y_train = y_train.astype(np.int32)\n",
        "    y_test = y_test.astype(np.int32)\n",
        "    y_train_encoded = to_categorical(y_train, num_classes)\n",
        "    y_test_encoded = to_categorical(y_test, num_classes)\n",
        "\n",
        "    # Reshape the input data for CNN\n",
        "    input_shape = (X_train.shape[1], 1)\n",
        "    X_train = X_train.reshape(-1, X_train.shape[1], 1)\n",
        "    X_test = X_test.reshape(-1, X_test.shape[1], 1)\n",
        "\n",
        "    # Define the CNN architecture\n",
        "    model = models.Sequential([\n",
        "        layers.Conv1D(32, 3, activation='relu', input_shape=input_shape),\n",
        "        layers.MaxPooling1D(2),\n",
        "        layers.Conv1D(64, 3, activation='relu'),\n",
        "        layers.MaxPooling1D(2),\n",
        "        layers.Conv1D(64, 3, activation='relu'),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy', Precision(), Recall()])\n",
        "\n",
        "    # Train the model\n",
        "    start_time = time.time()\n",
        "    history = model.fit(X_train, y_train_encoded, epochs=10, batch_size=32, validation_data=(X_test, y_test_encoded))\n",
        "    end_time = time.time()\n",
        "    training_time = end_time - start_time\n",
        "\n",
        "    # Evaluate the model\n",
        "    loss, accuracy, precision, recall = model.evaluate(X_test, y_test_encoded)\n",
        "    y_pred_probs = model.predict(X_test)\n",
        "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "    y_test_array = np.argmax(y_test_encoded, axis=1)\n",
        "    f1 = f1_score(y_test_array, y_pred, average='weighted')\n",
        "\n",
        "    # Print metrics\n",
        "    print(\"Test Loss:\", loss)\n",
        "    print(\"Test Accuracy:\", accuracy)\n",
        "    print(\"Test Precision:\", precision)\n",
        "    print(\"Test Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1)\n",
        "    print(\"Training Time:\", training_time)\n",
        "\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "AH7RbTz8QdTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def multi_class_prediction(path):\n",
        "  input, output = data_preprocessing_multiclass(path)\n",
        "  input_standardized = standardize(input)\n",
        "  cnn_model_multiclass = multiclass_cnn(input_standardized, output)\n",
        "  return cnn_model_multiclass\n",
        "path = '/content/drive/MyDrive/5G NIDD/Combined.csv'\n",
        "model_multi = multi_class_prediction(path)"
      ],
      "metadata": {
        "id": "neT7om2jQfkZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}