{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "#to ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "-Jc6ytvmKcKF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "159b5b2c-796a-4e92-e9b0-58952275d494"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UQAM4XZFnsG"
      },
      "outputs": [],
      "source": [
        "def data_preprocessing_binary(path):\n",
        "  df = pd.read_csv(path)\n",
        "  df_encoded = pd.get_dummies(df[['Proto','State']], columns=['Proto', 'State'])\n",
        "\n",
        "  select_features_df = df[[ 'AckDat', 'sHops', 'Seq','TcpRtt', 'dMeanPktSz', 'Offset', 'sTtl',  'Mean', 'SrcTCPBase', 'sMeanPktSz', 'DstLoss', 'Loss', 'dTtl', 'SrcBytes', 'TotBytes']]\n",
        "  select_features_df['sHops'].fillna(df['sHops'].mean(), inplace = True)\n",
        "  select_features_df['sTtl'].fillna(df['sTtl'].mean(), inplace = True)\n",
        "  select_features_df['SrcTCPBase'].fillna(df['SrcTCPBase'].mean(), inplace = True)\n",
        "  select_features_df['dTtl'].fillna(df['dTtl'].mean(), inplace = True)\n",
        "  df_merged = pd.concat([select_features_df, df_encoded], axis=1)\n",
        "\n",
        "  df_output_encoded = df['Label']\n",
        "  df_output_encoded_1 = df_output_encoded.replace(['Benign', 'Malicious'],\n",
        "                        [0, 1], inplace=False)\n",
        "\n",
        "\n",
        "  #returns input and output dataframe\n",
        "  return df_merged, df_output_encoded_1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_cnn(X, y):\n",
        "  import tensorflow as tf\n",
        "  from tensorflow.keras import layers, models\n",
        "  from sklearn.model_selection import train_test_split\n",
        "  import numpy as np\n",
        "  from tensorflow.keras.models import Sequential\n",
        "  from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "  from tensorflow.keras.optimizers import Adam\n",
        "  from tensorflow.keras.metrics import Precision, Recall\n",
        "  from sklearn.metrics import f1_score\n",
        "  import time\n",
        "\n",
        "\n",
        "  # Assuming df_normalized is your preprocessed DataFrame\n",
        "  # Assuming df_output_encoded_1 contains the binary labels for classification\n",
        "\n",
        "  # Split the data into features and labels\n",
        "  X = X.values  # Features\n",
        "  y = y.values  # Labels\n",
        "\n",
        "  # Split the data into training and testing sets\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "  # Reshape the input data for CNN (assuming df_normalized has shape (n_samples, n_features))\n",
        "  input_shape = (X.shape[1], 1)  # Assuming you have n_features columns\n",
        "  X_train = X_train.reshape(-1, X_train.shape[1], 1)\n",
        "  X_test = X_test.reshape(-1, X_test.shape[1], 1)\n",
        "\n",
        "  # Define the CNN architecture\n",
        "  model = models.Sequential([\n",
        "      # Convolutional layers\n",
        "      layers.Conv1D(32, 3, activation='relu', input_shape=input_shape),\n",
        "      layers.MaxPooling1D(2),\n",
        "      layers.Conv1D(64, 3, activation='relu'),\n",
        "      layers.MaxPooling1D(2),\n",
        "      layers.Conv1D(64, 3, activation='relu'),\n",
        "\n",
        "      # Dense layers\n",
        "      layers.Flatten(),\n",
        "      layers.Dense(64, activation='relu'),\n",
        "      layers.Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
        "  ])\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(optimizer='adam',\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy', Precision(), Recall()])\n",
        "\n",
        "  start_time = time.time()\n",
        "  history = model.fit(X_train, y_train, epochs=1, batch_size=32, validation_data=(X_test, y_test))\n",
        "  end_time = time.time()\n",
        "  training_time = end_time - start_time\n",
        "\n",
        "  # Evaluate the model\n",
        "  loss, accuracy, precision, recall = model.evaluate(X_test, y_test)\n",
        "  y_pred = model.predict(X_test)\n",
        "  f1 = f1_score(y_test, y_pred > 0.5)\n",
        "\n",
        "  # Print metrics\n",
        "  print(\"Model Loss:\", loss)\n",
        "  print(\"Model Accuracy:\", accuracy)\n",
        "  print(\"Model Precision:\", precision)\n",
        "  print(\"Model Recall:\", recall)\n",
        "  print(\"F1 Score:\", f1)\n",
        "  print(\"Training Time:\", training_time)\n",
        "\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "4tCEuLOmOK5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_cnn_numpyarr(X, y):\n",
        "    # Import necessary libraries\n",
        "    import tensorflow as tf\n",
        "    from tensorflow.keras import layers, models\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.metrics import f1_score\n",
        "    import time\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Check the shape of X_train\n",
        "    print(\"Shape of X_train:\", X_train.shape)\n",
        "\n",
        "    # Reshape the input data for CNN (assuming X has shape (n_samples, n_features))\n",
        "    input_shape = (X_train.shape[1], 1)  # Assuming you have n_features columns\n",
        "    X_train = X_train.reshape(-1, X_train.shape[1], 1)\n",
        "    X_test = X_test.reshape(-1, X_test.shape[1], 1)\n",
        "\n",
        "    # Define the CNN architecture\n",
        "    model = models.Sequential([\n",
        "        # Convolutional layers\n",
        "        layers.Conv1D(32, 3, activation='relu', input_shape=input_shape),\n",
        "        layers.MaxPooling1D(2),\n",
        "        layers.Conv1D(64, 3, activation='relu'),\n",
        "\n",
        "        # Dense layers\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy', tf.metrics.Precision(), tf.metrics.Recall()])\n",
        "\n",
        "    # Start training time measurement\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(X_train, y_train, epochs=1, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "    # End training time measurement\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Calculate training time\n",
        "    training_time = end_time - start_time\n",
        "\n",
        "    # Evaluate the model\n",
        "    loss, accuracy, precision, recall = model.evaluate(X_test, y_test)\n",
        "    y_pred = model.predict(X_test)\n",
        "    f1 = f1_score(y_test, y_pred > 0.5)\n",
        "\n",
        "    # Print metrics\n",
        "    print(\"Model Loss:\", loss)\n",
        "    print(\"Model Accuracy:\", accuracy)\n",
        "    print(\"Model Precision:\", precision)\n",
        "    print(\"Model Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1)\n",
        "    print(\"Training Time:\", training_time)\n",
        "\n",
        "    return model,X_test, y_test, history"
      ],
      "metadata": {
        "id": "TgasyOfScpxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def standardize(input_df):\n",
        "  scaler = StandardScaler()\n",
        "  df_standardized = scaler.fit_transform(input_df)\n",
        "  df_standardized = pd.DataFrame(df_standardized, columns=input_df.columns)\n",
        "  return df_standardized\n"
      ],
      "metadata": {
        "id": "quHxJyo3LNhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "def normalize_data(input_df):\n",
        "    # Handling missing values\n",
        "    imputer = SimpleImputer(strategy='mean')\n",
        "    input_df_filled = pd.DataFrame(imputer.fit_transform(input_df), columns=input_df.columns)\n",
        "\n",
        "    # Feature scaling\n",
        "    scaler = StandardScaler()\n",
        "    input_scaled = pd.DataFrame(scaler.fit_transform(input_df_filled), columns=input_df.columns)\n",
        "\n",
        "    return input_scaled\n"
      ],
      "metadata": {
        "id": "0yL_1Y9RzK72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "\n",
        "def remove_outliers_zscore(input_df, threshold=3):\n",
        "    z_scores = np.abs(stats.zscore(input_df))\n",
        "    input_df_no_outliers = input_df[(z_scores < threshold).all(axis=1)]\n",
        "    return input_df_no_outliers"
      ],
      "metadata": {
        "id": "2Q7nkyigeVx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "def select_features(input_df, target, k=10):\n",
        "    selector = SelectKBest(score_func=f_classif, k=k)\n",
        "    X_selected = selector.fit_transform(input_df, target)\n",
        "    selected_features = input_df.columns[selector.get_support()]\n",
        "    return X_selected, selected_features"
      ],
      "metadata": {
        "id": "SYiUEgx8YxZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve, auc\n",
        "import itertools\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, classes):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.colorbar()\n",
        "    tick_marks = range(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], 'd'),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()"
      ],
      "metadata": {
        "id": "FXXQRGl3oRyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    y_pred_probs = model.predict(X_test)\n",
        "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "    plot_confusion_matrix(y_test, y_pred, classes=['Benign', 'Malicious'])\n"
      ],
      "metadata": {
        "id": "IuQ2x4yBo4CG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/5G NIDD/Combined.csv'\n",
        "input, output = data_preprocessing_binary(path)\n"
      ],
      "metadata": {
        "id": "Qk4eMbz9JkYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_standardized = standardize(input)"
      ],
      "metadata": {
        "id": "8wevgV2jKwpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalize_data = normalize_data(input)"
      ],
      "metadata": {
        "id": "BuCBL-6Tf0ay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_no_outliers = remove_outliers_zscore(normalize_data)"
      ],
      "metadata": {
        "id": "TmX8oG7dfJA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = output.iloc[input_no_outliers.index]"
      ],
      "metadata": {
        "id": "znoiUDMkiAee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_selected, selected_features = select_features(input_no_outliers, output)"
      ],
      "metadata": {
        "id": "VcLE2veYZCk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"input_select:\",pd.DataFrame(input_selected).head())"
      ],
      "metadata": {
        "id": "s2L7cDDCijnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model = binary_cnn(input_standardized, output)"
      ],
      "metadata": {
        "id": "Ks3CM1pTMh_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model,X_test, y_test, history = binary_cnn_numpyarr(input_selected, output)"
      ],
      "metadata": {
        "id": "AsGq-Ckgzhvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(cnn_model, X_test, y_test)"
      ],
      "metadata": {
        "id": "9IKqUCV2pOZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "def create_rnn_model(input_shape):\n",
        "    model = Sequential([\n",
        "        LSTM(64, input_shape=input_shape),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "F_yAiYSSd5gk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming X_train and X_test are your input features\n",
        "\n",
        "# Reshape the input data for the RNN model\n",
        "X_train_rnn = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test_rnn = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n"
      ],
      "metadata": {
        "id": "Phkeb7sid7F2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "# Define a custom transformer to incorporate RNN model into the pipeline\n",
        "class RNNWrapper(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, rnn_model):\n",
        "        self.rnn_model = rnn_model\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self.rnn_model.fit(X, y, epochs=10, batch_size=32)  # Train the RNN model\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return self.rnn_model.predict(X)\n"
      ],
      "metadata": {
        "id": "KLlKs5oid_Pe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define and compile the RNN model\n",
        "rnn_model = create_rnn_model(input_shape=(1, X_train.shape[1]))\n",
        "rnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the RNN model\n",
        "rnn_model.fit(X_train_rnn, y_train, epochs=10, batch_size=32, validation_data=(X_test_rnn, y_test))\n"
      ],
      "metadata": {
        "id": "DF3onuiheBxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on test data using the trained RNN model\n",
        "y_pred = rnn_model.predict(X_test_rnn)\n"
      ],
      "metadata": {
        "id": "qqxike5_kDpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Threshold the predictions\n",
        "threshold = 0.5\n",
        "y_pred_binary = (y_pred > threshold).astype(int)\n",
        "\n",
        "# Print the first few binary predictions\n",
        "print(\"Sample of binary predictions:\")\n",
        "print(y_pred_binary[:10])\n"
      ],
      "metadata": {
        "id": "RuKdk3cIlv6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Evaluate the predictions\n",
        "accuracy = accuracy_score(y_test, y_pred_binary)\n",
        "precision = precision_score(y_test, y_pred_binary)\n",
        "recall = recall_score(y_test, y_pred_binary)\n",
        "f1 = f1_score(y_test, y_pred_binary)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Evaluation Metrics:\")\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sQJFwlckc1_",
        "outputId": "2e148c8c-6fe4-4aff-e7c7-64b6d99418cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Metrics:\n",
            "Accuracy: 0.9995615530764359\n",
            "Precision: 0.9998283589205683\n",
            "Recall: 0.9993995138921985\n",
            "F1 Score: 0.999613890411612\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def data_preprocessing_multiclass(path):\n",
        "  from sklearn.preprocessing import LabelEncoder\n",
        "  df = pd.read_csv(path)\n",
        "  df_encoded = pd.get_dummies(df[['Proto','State']], columns=['Proto', 'State'])\n",
        "  df_encoded_output = pd.get_dummies(df[['Proto','State']], columns=['Proto', 'State'])\n",
        "  select_features_df = df[[ 'AckDat', 'sHops', 'Seq','TcpRtt', 'dMeanPktSz', 'Offset', 'sTtl',  'Mean', 'SrcTCPBase', 'sMeanPktSz', 'DstLoss', 'Loss', 'dTtl', 'SrcBytes', 'TotBytes']]\n",
        "  select_features_df['sHops'].fillna(df['sHops'].mean(), inplace = True)\n",
        "  select_features_df['sTtl'].fillna(df['sTtl'].mean(), inplace = True)\n",
        "  select_features_df['SrcTCPBase'].fillna(df['SrcTCPBase'].mean(), inplace = True)\n",
        "  select_features_df['dTtl'].fillna(df['dTtl'].mean(), inplace = True)\n",
        "  df_merged = pd.concat([select_features_df, df_encoded], axis=1)\n",
        "\n",
        "  # df_output_encoded = df['Label']\n",
        "  # df_output_encoded_1 = df_output_encoded.replace(['Benign', 'Malicious'],\n",
        "  #                       [0, 1], inplace=False)\n",
        "\n",
        "  label_encoder = LabelEncoder()\n",
        "  df_output_encoded = label_encoder.fit_transform(df['Attack Type'])\n",
        "\n",
        "  # Convert output encoded array into DataFrame\n",
        "  df_output_encoded_1 = pd.DataFrame(df_output_encoded, columns=['Attack_Type_Encoded'])\n",
        "  print(df_output_encoded_1)\n",
        "  #returns input and output dataframe\n",
        "  return df_merged, df_output_encoded_1"
      ],
      "metadata": {
        "id": "YDrVgftvTLWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def multiclass_cnn(X, y):\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    from tensorflow.keras import layers, models\n",
        "    from tensorflow.keras.utils import to_categorical\n",
        "    from tensorflow.keras.metrics import Precision, Recall\n",
        "    from sklearn.metrics import f1_score\n",
        "    import time\n",
        "\n",
        "    # Load the input data and labels\n",
        "    # Assuming X_input is your input DataFrame and y_output is your output DataFrame\n",
        "    X = X.values\n",
        "    y = y.values\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Standardize the input features\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    # Convert labels to integers and one-hot encode them\n",
        "    num_classes = len(np.unique(y))\n",
        "    y_train = y_train.astype(np.int32)\n",
        "    y_test = y_test.astype(np.int32)\n",
        "    y_train_encoded = to_categorical(y_train, num_classes)\n",
        "    y_test_encoded = to_categorical(y_test, num_classes)\n",
        "\n",
        "    # Reshape the input data for CNN\n",
        "    input_shape = (X_train.shape[1], 1)\n",
        "    X_train = X_train.reshape(-1, X_train.shape[1], 1)\n",
        "    X_test = X_test.reshape(-1, X_test.shape[1], 1)\n",
        "\n",
        "    # Define the CNN architecture\n",
        "    model = models.Sequential([\n",
        "        layers.Conv1D(32, 3, activation='relu', input_shape=input_shape),\n",
        "        layers.MaxPooling1D(2),\n",
        "        layers.Conv1D(64, 3, activation='relu'),\n",
        "        layers.MaxPooling1D(2),\n",
        "        layers.Conv1D(64, 3, activation='relu'),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy', Precision(), Recall()])\n",
        "\n",
        "    # Train the model\n",
        "    start_time = time.time()\n",
        "    history = model.fit(X_train, y_train_encoded, epochs=10, batch_size=32, validation_data=(X_test, y_test_encoded))\n",
        "    end_time = time.time()\n",
        "    training_time = end_time - start_time\n",
        "\n",
        "    # Evaluate the model\n",
        "    loss, accuracy, precision, recall = model.evaluate(X_test, y_test_encoded)\n",
        "    y_pred_probs = model.predict(X_test)\n",
        "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "    y_test_array = np.argmax(y_test_encoded, axis=1)\n",
        "    f1 = f1_score(y_test_array, y_pred, average='weighted')\n",
        "\n",
        "    # Print metrics\n",
        "    print(\"Test Loss:\", loss)\n",
        "    print(\"Test Accuracy:\", accuracy)\n",
        "    print(\"Test Precision:\", precision)\n",
        "    print(\"Test Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1)\n",
        "    print(\"Training Time:\", training_time)\n",
        "\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "AH7RbTz8QdTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def multi_class_prediction(path):\n",
        "  input, output = data_preprocessing_multiclass(path)\n",
        "  input_standardized = standardize(input)\n",
        "  cnn_model_multiclass = multiclass_cnn(input_standardized, output)\n",
        "  return cnn_model_multiclass\n",
        "path = '/content/drive/MyDrive/5G NIDD/Combined.csv'\n",
        "model_multi = multi_class_prediction(path)"
      ],
      "metadata": {
        "id": "neT7om2jQfkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_preprocessing_multiclass(path):\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    df = pd.read_csv(path)\n",
        "\n",
        "    # Selecting features\n",
        "    select_features_df = df[['AckDat', 'sHops', 'Seq', 'TcpRtt', 'dMeanPktSz', 'Offset', 'sTtl', 'Mean', 'SrcTCPBase', 'sMeanPktSz', 'DstLoss', 'Loss', 'dTtl', 'SrcBytes', 'TotBytes']]\n",
        "    select_features_df['sHops'].fillna(df['sHops'].mean(), inplace=True)\n",
        "    select_features_df['sTtl'].fillna(df['sTtl'].mean(), inplace=True)\n",
        "    select_features_df['SrcTCPBase'].fillna(df['SrcTCPBase'].mean(), inplace=True)\n",
        "    select_features_df['dTtl'].fillna(df['dTtl'].mean(), inplace=True)\n",
        "\n",
        "    # Extracting target variable\n",
        "    df_output = df['Attack Type']\n",
        "\n",
        "   # Label encoding target variable\n",
        "    label_encoder = LabelEncoder()\n",
        "    df_output_encoded = label_encoder.fit_transform(df_output)\n",
        "\n",
        "    # Splitting data into train and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(select_features_df, df_output_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Standardizing input features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_standardized = scaler.fit_transform(X_train)\n",
        "    X_test_standardized = scaler.transform(X_test)\n",
        "\n",
        "    # Reshape input data for LSTM\n",
        "    X_train_reshaped = X_train_standardized.reshape(X_train_standardized.shape[0], 1, X_train_standardized.shape[1])\n",
        "    X_test_reshaped = X_test_standardized.reshape(X_test_standardized.shape[0], 1, X_test_standardized.shape[1])\n",
        "\n",
        "    return X_train_reshaped, X_test_reshaped, y_train, y_test, label_encoder\n"
      ],
      "metadata": {
        "id": "FgzB3vN3Sxek"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
        "\n",
        "# Load and preprocess the data\n",
        "data_path = '/content/drive/MyDrive/5G NIDD/Combined.csv'  # Update with your file path\n",
        "\n",
        "# Preprocess data\n",
        "X_train, X_test, y_train, y_test, label_encoder = data_preprocessing_multiclass(data_path)\n",
        "\n",
        "# Print the shape of X_train\n",
        "print(X_train.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "OWgKOzCzItL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the RNN model\n",
        "def create_multiclass_rnn(input_shape, num_classes):\n",
        "    model = Sequential([\n",
        "        LSTM(units=64, input_shape=input_shape),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "# Print the shape of X_train\n",
        "print(X_train.shape)"
      ],
      "metadata": {
        "id": "ksdg6UXQL3EV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Determine the number of unique classes\n",
        "num_classes = len(np.unique(y_train))\n",
        "\n",
        "# Create the model\n",
        "input_shape = X_train.shape[1:]  # Shape without batch size\n",
        "rnn_model = create_multiclass_rnn(input_shape, num_classes)\n",
        "\n",
        "# Compile the model\n",
        "rnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "print(rnn_model.summary())\n",
        "\n",
        "# Train the model\n",
        "rnn_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "yPA_-mLmLCnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model performance\n",
        "loss, accuracy = rnn_model.evaluate(X_test, y_test)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)\n",
        "\n"
      ],
      "metadata": {
        "id": "CxDKKSR9ccgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# Evaluate model performance\n",
        "y_pred = rnn_model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred_classes)\n",
        "precision = precision_score(y_test, y_pred_classes, average='weighted')\n",
        "recall = recall_score(y_test, y_pred_classes, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred_classes, average='weighted')\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)"
      ],
      "metadata": {
        "id": "qvnZmv6Wc9vY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Get the malicious categories list from the label encoder\n",
        "malicious_categories = label_encoder.classes_[label_encoder.classes_ != 'Benign']\n",
        "\n",
        "# Display classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "classification_rep = classification_report(y_test, y_pred_classes, target_names=label_encoder.classes_)\n",
        "\n",
        "# Print classification report\n",
        "print(classification_rep)\n"
      ],
      "metadata": {
        "id": "X_gfJWMy0gjz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}