{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "#to ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/',force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Jc6ytvmKcKF",
        "outputId": "622e82fd-d958-4ff0-e7a6-4573a3481d92"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4UQAM4XZFnsG"
      },
      "outputs": [],
      "source": [
        "def data_preprocessing_binary(path):\n",
        "  df = pd.read_csv(path)\n",
        "  df_encoded = pd.get_dummies(df[['Proto','State']], columns=['Proto', 'State'])\n",
        "\n",
        "  select_features_df = df[[ 'AckDat', 'sHops', 'Seq','TcpRtt', 'dMeanPktSz', 'Offset', 'sTtl',  'Mean', 'SrcTCPBase', 'sMeanPktSz', 'DstLoss', 'Loss', 'dTtl', 'SrcBytes', 'TotBytes']]\n",
        "  select_features_df['sHops'].fillna(df['sHops'].mean(), inplace = True)\n",
        "  select_features_df['sTtl'].fillna(df['sTtl'].mean(), inplace = True)\n",
        "  select_features_df['SrcTCPBase'].fillna(df['SrcTCPBase'].mean(), inplace = True)\n",
        "  select_features_df['dTtl'].fillna(df['dTtl'].mean(), inplace = True)\n",
        "  df_merged = pd.concat([select_features_df, df_encoded], axis=1)\n",
        "\n",
        "  df_output_encoded = df['Label']\n",
        "  df_output_encoded_1 = df_output_encoded.replace(['Benign', 'Malicious'],\n",
        "                        [0, 1], inplace=False)\n",
        "\n",
        "\n",
        "  #returns input and output dataframe\n",
        "  return df_merged, df_output_encoded_1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def data_preprocessing_multiclass(path):\n",
        "  from sklearn.preprocessing import LabelEncoder\n",
        "  df = pd.read_csv(path)\n",
        "  df_encoded = pd.get_dummies(df[['Proto','State']], columns=['Proto', 'State'])\n",
        "  df_encoded_output = pd.get_dummies(df[['Proto','State']], columns=['Proto', 'State'])\n",
        "  select_features_df = df[[ 'AckDat', 'sHops', 'Seq','TcpRtt', 'dMeanPktSz', 'Offset', 'sTtl',  'Mean', 'SrcTCPBase', 'sMeanPktSz', 'DstLoss', 'Loss', 'dTtl', 'SrcBytes', 'TotBytes']]\n",
        "  select_features_df['sHops'].fillna(df['sHops'].mean(), inplace = True)\n",
        "  select_features_df['sTtl'].fillna(df['sTtl'].mean(), inplace = True)\n",
        "  select_features_df['SrcTCPBase'].fillna(df['SrcTCPBase'].mean(), inplace = True)\n",
        "  select_features_df['dTtl'].fillna(df['dTtl'].mean(), inplace = True)\n",
        "  df_merged = pd.concat([select_features_df, df_encoded], axis=1)\n",
        "\n",
        "  # df_output_encoded = df['Label']\n",
        "  # df_output_encoded_1 = df_output_encoded.replace(['Benign', 'Malicious'],\n",
        "  #                       [0, 1], inplace=False)\n",
        "\n",
        "  label_encoder = LabelEncoder()\n",
        "  df_output_encoded = label_encoder.fit_transform(df['Attack Type'])\n",
        "\n",
        "  # Convert output encoded array into DataFrame\n",
        "  df_output_encoded_1 = pd.DataFrame(df_output_encoded, columns=['Attack_Type_Encoded'])\n",
        "  print(df_output_encoded_1)\n",
        "  #returns input and output dataframe\n",
        "  return df_merged, df_output_encoded_1"
      ],
      "metadata": {
        "id": "crP9NLrcgECO"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def multiclass_cnn(X, y):\n",
        "    import tensorflow as tf\n",
        "    from tensorflow.keras import layers, models\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    import numpy as np\n",
        "    from tensorflow.keras.optimizers import Adam\n",
        "    from tensorflow.keras.metrics import Precision, Recall, CategoricalAccuracy\n",
        "    from sklearn.metrics import f1_score\n",
        "    import time\n",
        "\n",
        "    # Split the data into features and labels\n",
        "    X = X.values  # Features\n",
        "    y = y.values  # Labels\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Reshape the input data for CNN\n",
        "    input_shape = (X.shape[1], 1)\n",
        "    X_train = X_train.reshape(-1, X_train.shape[1], 1)\n",
        "    X_test = X_test.reshape(-1, X_test.shape[1], 1)\n",
        "\n",
        "    # Determine the number of classes\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    # Define the CNN architecture\n",
        "    model = models.Sequential([\n",
        "        # Convolutional layers\n",
        "        layers.Conv1D(32, 3, activation='relu', input_shape=input_shape),\n",
        "        layers.MaxPooling1D(2),\n",
        "        layers.Conv1D(64, 3, activation='relu'),\n",
        "        layers.MaxPooling1D(2),\n",
        "        layers.Conv1D(64, 3, activation='relu'),\n",
        "\n",
        "        # Dense layers\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(num_classes, activation='softmax')  # Output layer for multiclass classification\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy', Precision(), Recall(), CategoricalAccuracy()])\n",
        "\n",
        "    start_time = time.time()\n",
        "    history = model.fit(X_train, y_train, epochs=1, batch_size=32, validation_data=(X_test, y_test))\n",
        "    end_time = time.time()\n",
        "    training_time = end_time - start_time\n",
        "\n",
        "    # Evaluate the model\n",
        "    loss, accuracy, precision, recall, cat_accuracy = model.evaluate(X_test, y_test)\n",
        "    y_pred = np.argmax(model.predict(X_test), axis=-1)\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    # Print metrics\n",
        "    print(\"Model Loss:\", loss)\n",
        "    print(\"Model Accuracy:\", accuracy)\n",
        "    print(\"Model Precision:\", precision)\n",
        "    print(\"Model Recall:\", recall)\n",
        "    print(\"Categorical Accuracy:\", cat_accuracy)\n",
        "    print(\"F1 Score:\", f1)\n",
        "    print(\"Training Time:\", training_time)\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "4tCEuLOmOK5k"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def multiclass_cnn(X, y):\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    from tensorflow.keras import layers, models\n",
        "    from tensorflow.keras.utils import to_categorical\n",
        "    from tensorflow.keras.metrics import Precision, Recall\n",
        "    from sklearn.metrics import f1_score\n",
        "    import time\n",
        "\n",
        "    # Load the input data and labels\n",
        "    # Assuming X_input is your input DataFrame and y_output is your output DataFrame\n",
        "    X = input_standardized.values\n",
        "    y = output.values\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Standardize the input features\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    # Convert labels to integers and one-hot encode them\n",
        "    num_classes = len(np.unique(y))\n",
        "    y_train = y_train.astype(np.int32)\n",
        "    y_test = y_test.astype(np.int32)\n",
        "    y_train_encoded = to_categorical(y_train, num_classes)\n",
        "    y_test_encoded = to_categorical(y_test, num_classes)\n",
        "\n",
        "    # Reshape the input data for CNN\n",
        "    input_shape = (X_train.shape[1], 1)\n",
        "    X_train = X_train.reshape(-1, X_train.shape[1], 1)\n",
        "    X_test = X_test.reshape(-1, X_test.shape[1], 1)\n",
        "\n",
        "    # Define the CNN architecture\n",
        "    model = models.Sequential([\n",
        "        layers.Conv1D(32, 3, activation='relu', input_shape=input_shape),\n",
        "        layers.MaxPooling1D(2),\n",
        "        layers.Conv1D(64, 3, activation='relu'),\n",
        "        layers.MaxPooling1D(2),\n",
        "        layers.Conv1D(64, 3, activation='relu'),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy', Precision(), Recall()])\n",
        "\n",
        "    # Train the model\n",
        "    start_time = time.time()\n",
        "    history = model.fit(X_train, y_train_encoded, epochs=10, batch_size=32, validation_data=(X_test, y_test_encoded))\n",
        "    end_time = time.time()\n",
        "    training_time = end_time - start_time\n",
        "\n",
        "    # Evaluate the model\n",
        "    loss, accuracy, precision, recall = model.evaluate(X_test, y_test_encoded)\n",
        "    y_pred_probs = model.predict(X_test)\n",
        "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "    y_test_array = np.argmax(y_test_encoded, axis=1)\n",
        "    f1 = f1_score(y_test_array, y_pred, average='weighted')\n",
        "\n",
        "    # Print metrics\n",
        "    print(\"Test Loss:\", loss)\n",
        "    print(\"Test Accuracy:\", accuracy)\n",
        "    print(\"Test Precision:\", precision)\n",
        "    print(\"Test Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1)\n",
        "    print(\"Training Time:\", training_time)\n",
        "\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "CZZYktsyjn4x"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def standardize(input_df):\n",
        "  scaler = StandardScaler()\n",
        "  df_standardized = scaler.fit_transform(input_df)\n",
        "  df_standardized = pd.DataFrame(df_standardized, columns=input_df.columns)\n",
        "  return df_standardized\n"
      ],
      "metadata": {
        "id": "quHxJyo3LNhS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "def normalize_data(input_df):\n",
        "    # Handling missing values\n",
        "    imputer = SimpleImputer(strategy='mean')\n",
        "    input_df_filled = pd.DataFrame(imputer.fit_transform(input_df), columns=input_df.columns)\n",
        "\n",
        "    # Feature scaling\n",
        "    scaler = StandardScaler()\n",
        "    input_scaled = pd.DataFrame(scaler.fit_transform(input_df_filled), columns=input_df.columns)\n",
        "\n",
        "    return input_scaled\n"
      ],
      "metadata": {
        "id": "0yL_1Y9RzK72"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/5G NIDD/Combined.csv'\n",
        "input, output = data_preprocessing_binary(path)\n"
      ],
      "metadata": {
        "id": "Qk4eMbz9JkYx"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_standardized = standardize(input)"
      ],
      "metadata": {
        "id": "8wevgV2jKwpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalize_data = normalize_data(input)"
      ],
      "metadata": {
        "id": "HJGdcjHOzR6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model = binary_cnn(input_standardized, output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ks3CM1pTMh_M",
        "outputId": "51956e7a-1452-4561-8884-14ef556ecfad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30398/30398 [==============================] - 236s 8ms/step - loss: 0.0087 - accuracy: 0.9976 - precision_2: 0.9977 - recall_2: 0.9983 - val_loss: 0.0050 - val_accuracy: 0.9983 - val_precision_2: 0.9990 - val_recall_2: 0.9982\n",
            "7600/7600 [==============================] - 25s 3ms/step - loss: 0.0050 - accuracy: 0.9983 - precision_2: 0.9990 - recall_2: 0.9982\n",
            "7600/7600 [==============================] - 22s 3ms/step\n",
            "Model Loss: 0.005041345488280058\n",
            "Model Accuracy: 0.9982770085334778\n",
            "Model Precision: 0.9989542961120605\n",
            "Model Recall: 0.9982019662857056\n",
            "F1 Score: 0.9985780076495722\n",
            "Training Time: 236.9183702468872\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model = binary_cnn(normalize_data, output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsGq-Ckgzhvf",
        "outputId": "021109d8-3afd-47de-8748-23b58f8edadd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30398/30398 [==============================] - 109s 4ms/step - loss: 0.0085 - accuracy: 0.9977 - precision: 0.9978 - recall: 0.9983 - val_loss: 0.0036 - val_accuracy: 0.9988 - val_precision: 0.9986 - val_recall: 0.9995\n",
            "7600/7600 [==============================] - 12s 2ms/step - loss: 0.0036 - accuracy: 0.9988 - precision: 0.9986 - recall: 0.9995\n",
            "7600/7600 [==============================] - 10s 1ms/step\n",
            "Model Loss: 0.0035908641293644905\n",
            "Model Accuracy: 0.9988033175468445\n",
            "Model Precision: 0.9985628724098206\n",
            "Model Recall: 0.9994639754295349\n",
            "F1 Score: 0.999013228168097\n",
            "Training Time: 143.24730443954468\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YDrVgftvTLWZ"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BXH0FpPMoMn7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}