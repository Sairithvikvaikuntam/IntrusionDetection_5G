{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "#to ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Jc6ytvmKcKF",
        "outputId": "1695d82d-275a-4299-c247-1db41b38f082"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "4UQAM4XZFnsG"
      },
      "outputs": [],
      "source": [
        "def data_preprocessing_binary(path):\n",
        "  df = pd.read_csv(path)\n",
        "  df_encoded = pd.get_dummies(df[['Proto','State']], columns=['Proto', 'State'])\n",
        "\n",
        "  select_features_df = df[[ 'AckDat', 'sHops', 'Seq','TcpRtt', 'dMeanPktSz', 'Offset', 'sTtl',  'Mean', 'SrcTCPBase', 'sMeanPktSz', 'DstLoss', 'Loss', 'dTtl', 'SrcBytes', 'TotBytes']]\n",
        "  select_features_df['sHops'].fillna(df['sHops'].mean(), inplace = True)\n",
        "  select_features_df['sTtl'].fillna(df['sTtl'].mean(), inplace = True)\n",
        "  select_features_df['SrcTCPBase'].fillna(df['SrcTCPBase'].mean(), inplace = True)\n",
        "  select_features_df['dTtl'].fillna(df['dTtl'].mean(), inplace = True)\n",
        "  df_merged = pd.concat([select_features_df, df_encoded], axis=1)\n",
        "\n",
        "  df_output_encoded = df['Label']\n",
        "  df_output_encoded_1 = df_output_encoded.replace(['Benign', 'Malicious'],\n",
        "                        [0, 1], inplace=False)\n",
        "\n",
        "\n",
        "  #returns input and output dataframe\n",
        "  return df_merged, df_output_encoded_1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_cnn(X, y):\n",
        "  import tensorflow as tf\n",
        "  from tensorflow.keras import layers, models\n",
        "  from sklearn.model_selection import train_test_split\n",
        "  import numpy as np\n",
        "  from tensorflow.keras.models import Sequential\n",
        "  from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "  from tensorflow.keras.optimizers import Adam\n",
        "  from tensorflow.keras.metrics import Precision, Recall\n",
        "  from sklearn.metrics import f1_score\n",
        "  import time\n",
        "\n",
        "\n",
        "  # Assuming df_normalized is your preprocessed DataFrame\n",
        "  # Assuming df_output_encoded_1 contains the binary labels for classification\n",
        "\n",
        "  # Split the data into features and labels\n",
        "  X = X.values  # Features\n",
        "  y = y.values  # Labels\n",
        "\n",
        "  # Split the data into training and testing sets\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "  # Reshape the input data for CNN (assuming df_normalized has shape (n_samples, n_features))\n",
        "  input_shape = (X.shape[1], 1)  # Assuming you have n_features columns\n",
        "  X_train = X_train.reshape(-1, X_train.shape[1], 1)\n",
        "  X_test = X_test.reshape(-1, X_test.shape[1], 1)\n",
        "\n",
        "  # Define the CNN architecture\n",
        "  model = models.Sequential([\n",
        "      # Convolutional layers\n",
        "      layers.Conv1D(32, 3, activation='relu', input_shape=input_shape),\n",
        "      layers.MaxPooling1D(2),\n",
        "      layers.Conv1D(64, 3, activation='relu'),\n",
        "      layers.MaxPooling1D(2),\n",
        "      layers.Conv1D(64, 3, activation='relu'),\n",
        "\n",
        "      # Dense layers\n",
        "      layers.Flatten(),\n",
        "      layers.Dense(64, activation='relu'),\n",
        "      layers.Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
        "  ])\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(optimizer='adam',\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy', Precision(), Recall()])\n",
        "\n",
        "  start_time = time.time()\n",
        "  history = model.fit(X_train, y_train, epochs=1, batch_size=32, validation_data=(X_test, y_test))\n",
        "  end_time = time.time()\n",
        "  training_time = end_time - start_time\n",
        "\n",
        "  # Evaluate the model\n",
        "  loss, accuracy, precision, recall = model.evaluate(X_test, y_test)\n",
        "  y_pred = model.predict(X_test)\n",
        "  f1 = f1_score(y_test, y_pred > 0.5)\n",
        "\n",
        "  # Print metrics\n",
        "  print(\"Model Loss:\", loss)\n",
        "  print(\"Model Accuracy:\", accuracy)\n",
        "  print(\"Model Precision:\", precision)\n",
        "  print(\"Model Recall:\", recall)\n",
        "  print(\"F1 Score:\", f1)\n",
        "  print(\"Training Time:\", training_time)\n",
        "\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "4tCEuLOmOK5k"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def standardize(input_df):\n",
        "  scaler = StandardScaler()\n",
        "  df_standardized = scaler.fit_transform(input_df)\n",
        "  df_standardized = pd.DataFrame(df_standardized, columns=input_df.columns)\n",
        "  return df_standardized\n"
      ],
      "metadata": {
        "id": "quHxJyo3LNhS"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "def normalize_data(input_df):\n",
        "    # Handling missing values\n",
        "    imputer = SimpleImputer(strategy='mean')\n",
        "    input_df_filled = pd.DataFrame(imputer.fit_transform(input_df), columns=input_df.columns)\n",
        "\n",
        "    # Feature scaling\n",
        "    scaler = StandardScaler()\n",
        "    input_scaled = pd.DataFrame(scaler.fit_transform(input_df_filled), columns=input_df.columns)\n",
        "\n",
        "    return input_scaled\n"
      ],
      "metadata": {
        "id": "0yL_1Y9RzK72"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "def remove_outliers_zscore(input_df, threshold=3):\n",
        "    z_scores = np.abs(stats.zscore(input_df))\n",
        "    input_df_no_outliers = input_df[(z_scores < threshold).all(axis=1)]\n",
        "    return input_df_no_outliers"
      ],
      "metadata": {
        "id": "2Q7nkyigeVx0"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/5G NIDD/Combined.csv'\n",
        "input, output = data_preprocessing_binary(path)\n"
      ],
      "metadata": {
        "id": "Qk4eMbz9JkYx"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_standardized = standardize(input)"
      ],
      "metadata": {
        "id": "8wevgV2jKwpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalize_data = normalize_data(input)"
      ],
      "metadata": {
        "id": "BuCBL-6Tf0ay"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_no_outliers = remove_outliers_zscore(normalize_data)"
      ],
      "metadata": {
        "id": "TmX8oG7dfJA-"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = output.iloc[input_no_outliers.index]"
      ],
      "metadata": {
        "id": "znoiUDMkiAee"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model = binary_cnn(input_standardized, output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ks3CM1pTMh_M",
        "outputId": "51956e7a-1452-4561-8884-14ef556ecfad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30398/30398 [==============================] - 236s 8ms/step - loss: 0.0087 - accuracy: 0.9976 - precision_2: 0.9977 - recall_2: 0.9983 - val_loss: 0.0050 - val_accuracy: 0.9983 - val_precision_2: 0.9990 - val_recall_2: 0.9982\n",
            "7600/7600 [==============================] - 25s 3ms/step - loss: 0.0050 - accuracy: 0.9983 - precision_2: 0.9990 - recall_2: 0.9982\n",
            "7600/7600 [==============================] - 22s 3ms/step\n",
            "Model Loss: 0.005041345488280058\n",
            "Model Accuracy: 0.9982770085334778\n",
            "Model Precision: 0.9989542961120605\n",
            "Model Recall: 0.9982019662857056\n",
            "F1 Score: 0.9985780076495722\n",
            "Training Time: 236.9183702468872\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model = binary_cnn(input_no_outliers, output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsGq-Ckgzhvf",
        "outputId": "c1994824-6a56-4aea-921b-20fa1a1fe119"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23093/23093 [==============================] - 183s 8ms/step - loss: 0.0067 - accuracy: 0.9983 - precision: 0.9986 - recall: 0.9985 - val_loss: 0.0016 - val_accuracy: 0.9994 - val_precision: 0.9999 - val_recall: 0.9991\n",
            "5774/5774 [==============================] - 19s 3ms/step - loss: 0.0016 - accuracy: 0.9994 - precision: 0.9999 - recall: 0.9991\n",
            "5774/5774 [==============================] - 18s 3ms/step\n",
            "Model Loss: 0.0016002435004338622\n",
            "Model Accuracy: 0.9994370341300964\n",
            "Model Precision: 0.9999427795410156\n",
            "Model Recall: 0.9990659356117249\n",
            "F1 Score: 0.9995041432644537\n",
            "Training Time: 183.82551646232605\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YDrVgftvTLWZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}