{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "#to ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Jc6ytvmKcKF",
        "outputId": "29522e3a-06ba-48c0-b6cd-5b6e5a90b0d1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UQAM4XZFnsG"
      },
      "outputs": [],
      "source": [
        "def data_preprocessing_binary(path):\n",
        "  df = pd.read_csv(path)\n",
        "  df_encoded = pd.get_dummies(df[['Proto','State']], columns=['Proto', 'State'])\n",
        "\n",
        "  select_features_df = df[[ 'AckDat', 'sHops', 'Seq','TcpRtt', 'dMeanPktSz', 'Offset', 'sTtl',  'Mean', 'SrcTCPBase', 'sMeanPktSz', 'DstLoss', 'Loss', 'dTtl', 'SrcBytes', 'TotBytes']]\n",
        "  select_features_df['sHops'].fillna(df['sHops'].mean(), inplace = True)\n",
        "  select_features_df['sTtl'].fillna(df['sTtl'].mean(), inplace = True)\n",
        "  select_features_df['SrcTCPBase'].fillna(df['SrcTCPBase'].mean(), inplace = True)\n",
        "  select_features_df['dTtl'].fillna(df['dTtl'].mean(), inplace = True)\n",
        "  df_merged = pd.concat([select_features_df, df_encoded], axis=1)\n",
        "\n",
        "  df_output_encoded = df['Label']\n",
        "  df_output_encoded_1 = df_output_encoded.replace(['Benign', 'Malicious'],\n",
        "                        [0, 1], inplace=False)\n",
        "\n",
        "\n",
        "  #returns input and output dataframe\n",
        "  return df_merged, df_output_encoded_1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_cnn(X, y):\n",
        "  import tensorflow as tf\n",
        "  from tensorflow.keras import layers, models\n",
        "  from sklearn.model_selection import train_test_split\n",
        "  import numpy as np\n",
        "  from tensorflow.keras.models import Sequential\n",
        "  from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "  from tensorflow.keras.optimizers import Adam\n",
        "  from tensorflow.keras.metrics import Precision, Recall\n",
        "  from sklearn.metrics import f1_score\n",
        "  import time\n",
        "\n",
        "\n",
        "  # Assuming df_normalized is your preprocessed DataFrame\n",
        "  # Assuming df_output_encoded_1 contains the binary labels for classification\n",
        "\n",
        "  # Split the data into features and labels\n",
        "  X = X.values  # Features\n",
        "  y = y.values  # Labels\n",
        "\n",
        "  # Split the data into training and testing sets\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "  # Reshape the input data for CNN (assuming df_normalized has shape (n_samples, n_features))\n",
        "  input_shape = (X.shape[1], 1)  # Assuming you have n_features columns\n",
        "  X_train = X_train.reshape(-1, X_train.shape[1], 1)\n",
        "  X_test = X_test.reshape(-1, X_test.shape[1], 1)\n",
        "\n",
        "  # Define the CNN architecture\n",
        "  model = models.Sequential([\n",
        "      # Convolutional layers\n",
        "      layers.Conv1D(32, 3, activation='relu', input_shape=input_shape),\n",
        "      layers.MaxPooling1D(2),\n",
        "      layers.Conv1D(64, 3, activation='relu'),\n",
        "      layers.MaxPooling1D(2),\n",
        "      layers.Conv1D(64, 3, activation='relu'),\n",
        "\n",
        "      # Dense layers\n",
        "      layers.Flatten(),\n",
        "      layers.Dense(64, activation='relu'),\n",
        "      layers.Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
        "  ])\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(optimizer='adam',\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy', Precision(), Recall()])\n",
        "\n",
        "  start_time = time.time()\n",
        "  history = model.fit(X_train, y_train, epochs=1, batch_size=32, validation_data=(X_test, y_test))\n",
        "  end_time = time.time()\n",
        "  training_time = end_time - start_time\n",
        "\n",
        "  # Evaluate the model\n",
        "  loss, accuracy, precision, recall = model.evaluate(X_test, y_test)\n",
        "  y_pred = model.predict(X_test)\n",
        "  f1 = f1_score(y_test, y_pred > 0.5)\n",
        "\n",
        "  # Print metrics\n",
        "  print(\"Model Loss:\", loss)\n",
        "  print(\"Model Accuracy:\", accuracy)\n",
        "  print(\"Model Precision:\", precision)\n",
        "  print(\"Model Recall:\", recall)\n",
        "  print(\"F1 Score:\", f1)\n",
        "  print(\"Training Time:\", training_time)\n",
        "\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "4tCEuLOmOK5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def standardize(input_df):\n",
        "  scaler = StandardScaler()\n",
        "  df_standardized = scaler.fit_transform(input_df)\n",
        "  df_standardized = pd.DataFrame(df_standardized, columns=input_df.columns)\n",
        "  return df_standardized\n"
      ],
      "metadata": {
        "id": "quHxJyo3LNhS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "def normalize_data(input_df):\n",
        "    # Handling missing values\n",
        "    imputer = SimpleImputer(strategy='mean')\n",
        "    input_df_filled = pd.DataFrame(imputer.fit_transform(input_df), columns=input_df.columns)\n",
        "\n",
        "    # Feature scaling\n",
        "    scaler = StandardScaler()\n",
        "    input_scaled = pd.DataFrame(scaler.fit_transform(input_df_filled), columns=input_df.columns)\n",
        "\n",
        "    return input_scaled\n"
      ],
      "metadata": {
        "id": "0yL_1Y9RzK72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "def remove_outliers_zscore(input_df, threshold=3):\n",
        "    z_scores = np.abs(stats.zscore(input_df))\n",
        "    input_df_no_outliers = input_df[(z_scores < threshold).all(axis=1)]\n",
        "    return input_df_no_outliers"
      ],
      "metadata": {
        "id": "2Q7nkyigeVx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/5G NIDD/Combined.csv'\n",
        "input, output = data_preprocessing_binary(path)\n"
      ],
      "metadata": {
        "id": "Qk4eMbz9JkYx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "0f92e84f-0645-4df9-9413-62b98862bae2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'data_preprocessing_binary' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-3c06d8806b65>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/5G NIDD/Combined.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_preprocessing_binary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'data_preprocessing_binary' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_standardized = standardize(input)"
      ],
      "metadata": {
        "id": "8wevgV2jKwpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalize_data = normalize_data(input)"
      ],
      "metadata": {
        "id": "BuCBL-6Tf0ay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_no_outliers = remove_outliers_zscore(normalize_data)"
      ],
      "metadata": {
        "id": "TmX8oG7dfJA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = output.iloc[input_no_outliers.index]"
      ],
      "metadata": {
        "id": "znoiUDMkiAee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model = binary_cnn(input_standardized, output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ks3CM1pTMh_M",
        "outputId": "51956e7a-1452-4561-8884-14ef556ecfad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30398/30398 [==============================] - 236s 8ms/step - loss: 0.0087 - accuracy: 0.9976 - precision_2: 0.9977 - recall_2: 0.9983 - val_loss: 0.0050 - val_accuracy: 0.9983 - val_precision_2: 0.9990 - val_recall_2: 0.9982\n",
            "7600/7600 [==============================] - 25s 3ms/step - loss: 0.0050 - accuracy: 0.9983 - precision_2: 0.9990 - recall_2: 0.9982\n",
            "7600/7600 [==============================] - 22s 3ms/step\n",
            "Model Loss: 0.005041345488280058\n",
            "Model Accuracy: 0.9982770085334778\n",
            "Model Precision: 0.9989542961120605\n",
            "Model Recall: 0.9982019662857056\n",
            "F1 Score: 0.9985780076495722\n",
            "Training Time: 236.9183702468872\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model = binary_cnn(input_no_outliers, output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsGq-Ckgzhvf",
        "outputId": "c1994824-6a56-4aea-921b-20fa1a1fe119"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23093/23093 [==============================] - 183s 8ms/step - loss: 0.0067 - accuracy: 0.9983 - precision: 0.9986 - recall: 0.9985 - val_loss: 0.0016 - val_accuracy: 0.9994 - val_precision: 0.9999 - val_recall: 0.9991\n",
            "5774/5774 [==============================] - 19s 3ms/step - loss: 0.0016 - accuracy: 0.9994 - precision: 0.9999 - recall: 0.9991\n",
            "5774/5774 [==============================] - 18s 3ms/step\n",
            "Model Loss: 0.0016002435004338622\n",
            "Model Accuracy: 0.9994370341300964\n",
            "Model Precision: 0.9999427795410156\n",
            "Model Recall: 0.9990659356117249\n",
            "F1 Score: 0.9995041432644537\n",
            "Training Time: 183.82551646232605\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def data_preprocessing_multiclass(path):\n",
        "  from sklearn.preprocessing import LabelEncoder\n",
        "  df = pd.read_csv(path)\n",
        "  df_encoded = pd.get_dummies(df[['Proto','State']], columns=['Proto', 'State'])\n",
        "  df_encoded_output = pd.get_dummies(df[['Proto','State']], columns=['Proto', 'State'])\n",
        "  select_features_df = df[[ 'AckDat', 'sHops', 'Seq','TcpRtt', 'dMeanPktSz', 'Offset', 'sTtl',  'Mean', 'SrcTCPBase', 'sMeanPktSz', 'DstLoss', 'Loss', 'dTtl', 'SrcBytes', 'TotBytes']]\n",
        "  select_features_df['sHops'].fillna(df['sHops'].mean(), inplace = True)\n",
        "  select_features_df['sTtl'].fillna(df['sTtl'].mean(), inplace = True)\n",
        "  select_features_df['SrcTCPBase'].fillna(df['SrcTCPBase'].mean(), inplace = True)\n",
        "  select_features_df['dTtl'].fillna(df['dTtl'].mean(), inplace = True)\n",
        "  df_merged = pd.concat([select_features_df, df_encoded], axis=1)\n",
        "\n",
        "  # df_output_encoded = df['Label']\n",
        "  # df_output_encoded_1 = df_output_encoded.replace(['Benign', 'Malicious'],\n",
        "  #                       [0, 1], inplace=False)\n",
        "\n",
        "  label_encoder = LabelEncoder()\n",
        "  df_output_encoded = label_encoder.fit_transform(df['Attack Type'])\n",
        "\n",
        "  # Convert output encoded array into DataFrame\n",
        "  df_output_encoded_1 = pd.DataFrame(df_output_encoded, columns=['Attack_Type_Encoded'])\n",
        "  print(df_output_encoded_1)\n",
        "  #returns input and output dataframe\n",
        "  return df_merged, df_output_encoded_1"
      ],
      "metadata": {
        "id": "X697glygA1z5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def multiclass_cnn(X, y):\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    from tensorflow.keras import layers, models\n",
        "    from tensorflow.keras.utils import to_categorical\n",
        "    from tensorflow.keras.metrics import Precision, Recall\n",
        "    from sklearn.metrics import f1_score\n",
        "    import time\n",
        "\n",
        "    # Load the input data and labels\n",
        "    # Assuming X_input is your input DataFrame and y_output is your output DataFrame\n",
        "    X = X.values\n",
        "    y = y.values\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Standardize the input features\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    # Convert labels to integers and one-hot encode them\n",
        "    num_classes = len(np.unique(y))\n",
        "    y_train = y_train.astype(np.int32)\n",
        "    y_test = y_test.astype(np.int32)\n",
        "    y_train_encoded = to_categorical(y_train, num_classes)\n",
        "    y_test_encoded = to_categorical(y_test, num_classes)\n",
        "\n",
        "    # Reshape the input data for CNN\n",
        "    input_shape = (X_train.shape[1], 1)\n",
        "    X_train = X_train.reshape(-1, X_train.shape[1], 1)\n",
        "    X_test = X_test.reshape(-1, X_test.shape[1], 1)\n",
        "\n",
        "    # Define the CNN architecture\n",
        "    model = models.Sequential([\n",
        "        layers.Conv1D(32, 3, activation='relu', input_shape=input_shape),\n",
        "        layers.MaxPooling1D(2),\n",
        "        layers.Conv1D(64, 3, activation='relu'),\n",
        "        layers.MaxPooling1D(2),\n",
        "        layers.Conv1D(64, 3, activation='relu'),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy', Precision(), Recall()])\n",
        "\n",
        "    # Train the model\n",
        "    start_time = time.time()\n",
        "    history = model.fit(X_train, y_train_encoded, epochs=10, batch_size=32, validation_data=(X_test, y_test_encoded))\n",
        "    end_time = time.time()\n",
        "    training_time = end_time - start_time\n",
        "\n",
        "    # Evaluate the model\n",
        "    loss, accuracy, precision, recall = model.evaluate(X_test, y_test_encoded)\n",
        "    y_pred_probs = model.predict(X_test)\n",
        "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "    y_test_array = np.argmax(y_test_encoded, axis=1)\n",
        "    f1 = f1_score(y_test_array, y_pred, average='weighted')\n",
        "\n",
        "    # Print metrics\n",
        "    print(\"Test Loss:\", loss)\n",
        "    print(\"Test Accuracy:\", accuracy)\n",
        "    print(\"Test Precision:\", precision)\n",
        "    print(\"Test Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1)\n",
        "    print(\"Training Time:\", training_time)\n",
        "\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "YDrVgftvTLWZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def multi_class_prediction(path):\n",
        "  input, output = data_preprocessing_multiclass(path)\n",
        "  input_standardized = standardize(input)\n",
        "  cnn_model_multiclass = multiclass_cnn(input_standardized, output)\n",
        "  return cnn_model_multiclass\n",
        "path = '/content/drive/MyDrive/5G NIDD/Combined.csv'\n",
        "model_multi = multi_class_prediction(path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZJiJY8vBc2C",
        "outputId": "48eba998-d72c-4f8a-b8aa-ca9f6c74471a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Attack_Type_Encoded\n",
            "0                          0\n",
            "1                          0\n",
            "2                          0\n",
            "3                          0\n",
            "4                          0\n",
            "...                      ...\n",
            "1215885                    0\n",
            "1215886                    0\n",
            "1215887                    0\n",
            "1215888                    0\n",
            "1215889                    0\n",
            "\n",
            "[1215890 rows x 1 columns]\n",
            "Epoch 1/10\n"
          ]
        }
      ]
    }
  ]
}